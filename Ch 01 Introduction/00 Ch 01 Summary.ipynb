{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ce9855",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction (Summary)\n",
    "\n",
    "<hr style=\"height:2px;\">   \n",
    "\n",
    "<h4> <li>Marginal Probability (Sum Rule)</h4>\n",
    "<ul>\n",
    "  <li> $P(X) = \\sum_{y} P(Y|X) P(X) \\;\\;\\;  (Discrete)$\n",
    "  <li> $P(X) = \\int_{y} P(Y|X) P(X) \\;\\;\\; (Continuous)$ \n",
    "\n",
    "</ul>\n",
    "\n",
    "<h4> <li>Joint Probability (Product Rule)</h4>\n",
    "    <ul>\n",
    "        <li> $P(X, Y) = P(Y|X) P(X) = P(X|Y) P(Y)$\n",
    "            </ul>\n",
    "<br>\n",
    "<li> <b>Discrete variable</b> => Probability mass function (PMF)\n",
    "<li> <b>Continuous variable</b> => Probability density function (PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1a4f4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">   \n",
    "\n",
    "<h4> The Gaussian distribution</h4>\n",
    "\n",
    "$$\n",
    "    \\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<li><b>For D-dimensional $X$:</b>\n",
    "    \n",
    "$$\n",
    "    \\mathcal{N}(x | \\mu, {\\sum}^{-1}) = \\frac{1}{{(2\\pi)}^{D/2}({\\det(\\sum)})^{1/2}}e^{-{1/2}{(x - \\mu)^T\\sum^{-1}(x - \\mu)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439de688",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">   \n",
    "\n",
    "<h4> MLE & MAP</h4><br>\n",
    "<li> In frequentist settings, w is considered to be fixed pararameters(unique), whose values is determined by some estimator.\n",
    "<li> The most widely used estimator is <b>Maximum Likelihood</b>. \n",
    "<li> w is parameter values that maximizes the likelihood function.\n",
    "<li> In ML, the -log of the likelihood function is the error function.</li>\n",
    "\n",
    "<h4> 1) Maximizing the likelihood function is equivalent to minimizing the error function.</h4>\n",
    "<br> \n",
    "    <ul>\n",
    "        <li> Assume the following:\n",
    "            <ul>\n",
    "                <li> $y = W^TX + \\epsilon$\n",
    "                <li> $\\epsilon$ is a random variable ~ $\\mathcal{N}(0, {\\sigma^2})$\n",
    "                <li> $P(y | w, x) = \\mathcal{N}(y | w^TX, \\sigma^2I)$\n",
    "            </ul><br>\n",
    "        <li> The likelihood function is:\n",
    "            <ul>\n",
    "                <li> $L(y_{1},..,y_{N}; w; X_{1},..,X_{N}) = \\prod_{n=1}^{N} \\mathcal{N}(y_{i} | w^TX_{i}, \\sigma^2I) = \\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{{2\\pi}^{N/2}\\sigma^N}e^{-\\frac{1}{2\\sigma^2}(\\sum_{N}(y_{i} - W^Tx_{i})^2)}$<br>\n",
    "                <li> So maximizing the liklihood is the same as minimizing the exponent, which is the error function(Least-mean-squares)\n",
    "                <li> Max L $\\equiv$ Min $\\sum_{N}(y_{i} - W^Tx_{i})^2$\n",
    "            </ul>\n",
    "    </ul>\n",
    "            \n",
    "<h4> 2) Maximizing a posterior is equivalent to minimizing the regularized error function.</h4>\n",
    "<br> \n",
    "    <ul>\n",
    "        <li> Assume the following:\n",
    "            <ul>\n",
    "                <li> $y = W^TX + \\epsilon$\n",
    "                <li> $\\epsilon$ is a random variable ~ $\\mathcal{N}(0, {\\sigma^2})$\n",
    "                <li> $P(w | t, x, \\beta, \\alpha) \\propto P(t | x, w, \\beta) P(w | \\alpha)$\n",
    "            </ul><br>\n",
    "        <li> MAP $\\equiv$ Min $\\sum_{N}(y_{i} - W^Tx_{i})^2 + (w-\\mu_{w}).T\\sum_{w}^{-1}(w-\\mu_{w})$\n",
    "            <br>\n",
    "        <li> After including $P(w | t, x, \\beta, \\alpha)$, we still making a point estimate of w </li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3d034",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">   \n",
    "\n",
    "<h4> Bayes' Theorem </h4><br>\n",
    "    <li> It's just manipulating a conditional probability.\n",
    "    <li> We have two events, observed data (D) and the parameters (w).<br>\n",
    "$$p(w | D) = \\frac{p(D|w)p(w)}{p(D)}$$\n",
    "<br>\n",
    "<ul>\n",
    "    <li> $P(w)$ : Information about w before observing D, it's our prior beliefs (domain knowledge).\n",
    "    <li> $P(D | w)$ : The liklihood function, describes the effect of D on w, it tells how likely the observed data for different settings of w.    \n",
    "    <li> $P(D)$ : The normalization constant, ensurs that the posterior distribution is a valid probability density. \n",
    "    <li> $P(w | D)$ : The posterior, reflects the probability of w after considering D. (the updated prior)   \n",
    "</ul>        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5181a",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">   \n",
    "\n",
    "<h4>Learning from data</h4>\n",
    "\n",
    "<b>1)</b> Bayesian statistics tries to preserve and reifne uncertainty by adjusting our domain knowledge in light of new data.<br>\n",
    "<b>2)</b> Frequentist statistics elimminate the uncertainty by providing an estimate(point estimate).\n",
    "        \n",
    "\n",
    "Given observed data $D=\\{(x_{i}, y_{i})\\}_{i=1}^N$. Dased on the observed data, find some way to predict new y given new x.<br>\n",
    "We have three main approaches to solve the decision problems:\n",
    "        \n",
    "<ul>\n",
    "    <li> <b>Generative models:</b>\n",
    "        <ul>\n",
    "            <li> Solve the inference problem to get $P(X , C_{k})$ for classification or $P(X , t)$ for regression.\n",
    "            <li> Apply Bayes' theorem to get the posterior $P(C_{k} | X) = \\frac{P(X, C_{k})}{P(X)}$ or $P(t | X) = \\frac{P(X, t)}{P(X)}$\n",
    "            <li> Apply the decision theory to find the class membership for classification.\n",
    "        </ul><br>\n",
    "    <li> <b>Discriminative models:</b>\n",
    "        <ul>\n",
    "            <li> Solve the inference problem to get $P(C_{k} |X )$ for classification or $P(t | X)$ for regression directly.\n",
    "            <li> No Bayes' theorem or $P(X)$.\n",
    "            <li> Apply the decision theory to find the class membership for classification.\n",
    "        </ul><br>\n",
    "    <li> <b>Discriminant function</b>\n",
    "        <ul>\n",
    "            <li> Find some function $F(X)$ that maps from $X$ to $C_{k}$ or $y$ directly.\n",
    "            <li> In this case, probabilities play no role.\n",
    "        </ul>\n",
    "        </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf3ace",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
