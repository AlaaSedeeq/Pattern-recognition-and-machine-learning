{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196d7bb3",
   "metadata": {},
   "source": [
    "# Chapter 4: Linear Models For Classification (Summary)\n",
    "\n",
    "<hr style=\"height:2px;\"></hr>\n",
    "\n",
    "<li> Classification problem:\n",
    "    <ul><li> Given $D=\\{(x_n, t_n)\\}_{n=1}^N$\n",
    "        <li> Assign the $D$-dimensional input $x$ to $C_k \\in \\{1, 2, .., K\\}$ by predicting $C_k$ or its posterior $P(C_k|x)$.\n",
    "        <li> The input space is then divided into regions, its boundaries are decision boundaries or decision surfaces\n",
    "    </ul>\n",
    "\n",
    "<li> We have three main approaches:\n",
    "    <ul><li><b>1) Discriminant function:</b>\n",
    "        <ul><li> Assign $x$ to $C_k$ directly\n",
    "            <li> Can not estimate the probability of the class $P(C_k|x)$\n",
    "            <li> ex. Perceptron\n",
    "        </ul>\n",
    "        <li><b>2) Discriminative Models:</b>\n",
    "        <ul><li> Model $P(C_k|x)$ directly.\n",
    "            <li> Can not generate new samples.\n",
    "            <li> ex. Logistic Regression\n",
    "        </ul>\n",
    "        <li><b>3) Generative Models:</b>\n",
    "        <ul><li> Model class-conditional densities $P(x|C_k)$ together with the prior $P(C_k)$.\n",
    "            <li> Then ues Bayes' rule to get the posterior $P(C_k|x)$.\n",
    "            <li> ex. Gaussian Discriminant Analysis.\n",
    "        </ul>\n",
    "    </ul>\n",
    "<li> Linear models is used in classification by considering a generalization form in which we transform the linear function of $w$ using a nonlinear function.\n",
    "    <ul><li> $y(x) = f(X^TW + W_0)$\n",
    "        <li> The function $f(.)$ is the activation function (in ML), its inverse is the link function (in statistics).\n",
    "        <li> The model is no longer linear in $w$ due to the non-linearity of $f(.)$.\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4bf32f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\"></hr>\n",
    "<h2>1) Discriminant Function</h2><br>\n",
    "\n",
    "<li> Assign $x => C_K$\n",
    "<li> All decision boundaries are linear, singly connected, and convex.\n",
    "<h3>For two-class problem:</h3>\n",
    "    <ul><li> $y = W^TX + W_0$\n",
    "        <li> $\\begin{equation}\n",
    "                C_K=\n",
    "                \\begin{cases}\n",
    "                1 & \\text{,if } y(x) \\geq 0\\\\\n",
    "                0 & \\text{,if } y(x) < 0\n",
    "                \\end{cases}\n",
    "                \\end{equation}$\n",
    "        <li> $W$ determine the orientation of the D.B.\n",
    "        <li> $W_0$ determines the location of the D.B.\n",
    "        <li> $y(x)$ gives a signed measure of the perpendicular distance of x from the D.B.\n",
    "    </ul>\n",
    "<h3>For multi-class problem:</h3>\n",
    "    <ul><li> Can be done by two main approaches:\n",
    "        <li>1) Using 2-class classifiers:\n",
    "            <ul><li> Leads to ambiguous.\n",
    "                <li> ex. One-Versus-The-Rest or One-Versus-One.\n",
    "            </ul>\n",
    "        <li>2) Using a single k-class discriminant comprising K linear functions\"\n",
    "            <ul><li> $y_k(x) = W^T_kX + W_{k0}$, $y(x) = (y_1(x),...,y_K(x))$.\n",
    "                <li> Assign $X$ to $C_K \\text{ where } y_k(x) > y_j(x) \\ \\ \\forall_{j\\neq k}$\n",
    "                <li> The D.B between $C_k$ and $C_j$ is $y_k(x) = y_j(x)$\n",
    "            </ul>\n",
    "    </ul>\n",
    "<h3>To find the parameters $W$ in discriminant function, we have three methods:</h3>\n",
    "    <ul><li><b>1) Least-Squares:</b>\n",
    "        <ul><li> $y(x) = \\tilde{W}^T\\tilde{X}$, where all classes models $y_k(x)$ are grouped.\n",
    "            <li> $E_D(\\tilde{w}) = \\frac{1}{2} \\mathrm{Tr}\\{(\\tilde{W}^T\\tilde{X} - T)^T(\\tilde{W}^T\\tilde{X} - T)\\}$, where $\\tilde{W} = (W_{k0}, W_{k})$ and $\\tilde{X} = (1, X)$\n",
    "            <li> $\\tilde{W} = (X^TX)^{-1}X^TT = X^{+}T$, where $T$ is $N x K$ matrix where $n^{th}$ row is $t_n^T$\n",
    "            <li> Too sensitive to outliers.\n",
    "        </ul>\n",
    "        <li><b>2) Fisher's Linear Discriminant:</b>\n",
    "            <ul><li> A way to view a linear classification model is in terms of dimensionality reduction.\n",
    "                <li> $ Y = W^TX$ is a projection from $X$-space to $Y$-space through the $W$\n",
    "                <li> We need to find the projection that maximizes the between-classes variance and minimizes the within-class variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9a011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0327376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37ea0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e777f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d3f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
