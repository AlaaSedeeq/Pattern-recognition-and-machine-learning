{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14812fb8",
   "metadata": {},
   "source": [
    "<!-- # Chapter 4: Linear Models For Classification (Summary)\n",
    "\n",
    "<li>\n",
    "    Classification problem:\n",
    "    <ul>\n",
    "        <li>Given $D=\\{(x_n, t_n)\\}_{n=1}^N$</li>\n",
    "        <li>Assign the $D$-dimensional input $x$ to $C_k \\in \\{1, 2, .., K\\}$ by predicting $C_k$ or its posterior $P(C_k|x)$.</li>\n",
    "        <li>The input space is then divided into regions, its boundaries are decision boundaries or decision surfaces</li>\n",
    "    </ul>\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "    We have three main approaches:\n",
    "    <ul>\n",
    "        <li>\n",
    "            <b>1) Discriminant function:</b>\n",
    "            <ul>\n",
    "                <li>Assign $x$ to $C_k$ directly</li>\n",
    "                <li>Can not estimate the probability of the class $P(C_k|x)$</li>\n",
    "                <li>ex. Perceptron</li>\n",
    "            </ul>\n",
    "\n",
    "\n",
    "        <li>\n",
    "            <b>2) Discriminative Models:</b>\n",
    "            <ul>\n",
    "                <li>Model $P(C_k|x)$ directly.</li>\n",
    "                <li>Can not generate new samples.</li>\n",
    "                <li>ex. Logistic Regression</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "\n",
    "        <li>\n",
    "            <b>3) Generative Models:</b>\n",
    "            <ul>\n",
    "                <li>Model class-conditional densities $P(x|C_k)$ together with the prior $P(C_k)$.</li>\n",
    "                <li>Then ues Bayes' rule to get the posterior $P(C_k|x)$.</li>\n",
    "                <li>ex. Gaussian Discriminant Analysis.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "    Linear models is used in classification by considering a generalization form in which we transform the linear function of $w$ using a nonlinear function.\n",
    "    <ul>\n",
    "        <li>$y(x) = f(X^TW + W_0)$</li>\n",
    "        <li>The function $f(.)$ is the activation function (in ML), its inverse is the link function (in statistics).</li>\n",
    "        <li>The model is no longer linear in $w$ due to the non-linearity of $f(.)$.</li>\n",
    "    </ul>\n",
    "</li>\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66884b76",
   "metadata": {},
   "source": [
    "# Chapter 4: Linear Models For Classification (Summary)\n",
    "\n",
    "<hr style=\"height:2px;\"></hr>\n",
    "\n",
    "Classification problem:\n",
    "Given $D=\\{(x_n, t_n)\\}_{n=1}^N$\n",
    "Assign the $D$-dimensional input $x$ to $C_k \\in \\{1, 2, .., K\\}$ by predicting $C_k$ or its posterior $P(C_k|x)$.\n",
    "The input space is then divided into regions, its boundaries are decision boundaries or decision surfaces\n",
    "\n",
    "We have three main approaches:\n",
    "- <b>1) Discriminant function:</b>\n",
    "    - Assign $x$ to $C_k$ directly\n",
    "    - Can not estimate the probability of the class $P(C_k|x)$\n",
    "    - ex. Perceptron\n",
    "\n",
    "\n",
    "- <b>2) Discriminative Models:</b>\n",
    "    - Model $P(C_k|x)$ directly.\n",
    "    - Can not generate new samples.\n",
    "    - ex. Logistic Regression\n",
    "    \n",
    "    \n",
    "- <b>3) Generative Models:</b>\n",
    "    - Model class-conditional densities $P(x|C_k)$ together with the prior $P(C_k)$.\n",
    "    - Then ues Bayes' rule to get the posterior $P(C_k|x)$.\n",
    "    - ex. Gaussian Discriminant Analysis.</li>\n",
    "    \n",
    "    \n",
    "- Linear models is used in classification by considering a generalization form in which we transform the linear function of $w$ using a nonlinear function.\n",
    "    - $y(x) = f(X^TW + W_0)$\n",
    "    - The function $f(.)$ is the activation function (in ML), its inverse is the link function (in statistics).\n",
    "    - The model is no longer linear in $w$ due to the non-linearity of $f(.)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da7823",
   "metadata": {},
   "source": [
    "<!-- <hr style=\"height:2px;\"></hr>\n",
    "\n",
    "<h2>1) Discriminant Function</h2><br>\n",
    "\n",
    "<li> Assign $x => C_K$\n",
    "<li> All decision boundaries are linear, singly connected, and convex.\n",
    "<h3>For two-class problem:</h3>\n",
    "    <ul><li> $y = W^TX + W_0$\n",
    "        <li> $\\begin{equation}\n",
    "                C_K=\n",
    "                \\begin{cases}\n",
    "                1 & \\text{,if } y(x) \\geq 0\\\\\n",
    "                0 & \\text{,if } y(x) < 0\n",
    "                \\end{cases}\n",
    "                \\end{equation}$</li>\n",
    "        <li> $W$ determine the orientation of the D.B.\n",
    "        <li> $W_0$ determines the location of the D.B.\n",
    "        <li> $y(x)$ gives a signed measure of the perpendicular distance of x from the D.B.\n",
    "    </ul>\n",
    "<h3>For multi-class problem:</h3>\n",
    "    <ul><li> Can be done by two main approaches:\n",
    "        <li>1) Using 2-class classifiers:\n",
    "            <ul><li> Leads to ambiguous.\n",
    "                <li> ex. One-Versus-The-Rest or One-Versus-One.\n",
    "            </ul>\n",
    "        <li>2) Using a single k-class discriminant comprising K linear functions\"\n",
    "            <ul><li> $y_k(x) = W^T_kX + W_{k0}$, $y(x) = (y_1(x),...,y_K(x))$.\n",
    "                <li> Assign $X$ to $C_K \\text{ where } y_k(x) > y_j(x) \\ \\ \\forall_{j\\neq k}$\n",
    "                <li> The D.B between $C_k$ and $C_j$ is $y_k(x) = y_j(x)$</li>\n",
    "            </ul>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<h3>To find the parameters $W$ in discriminant function, we have three methods:</h3>\n",
    "    <ul><li><b>1) Least-Squares:</b>\n",
    "        <ul><li> $y(x) = \\tilde{W}^T\\tilde{X}$, where $\\tilde{W} = (W_{k0}, W_{k})$ and $\\tilde{X} = (1, X)$ and all classes models $y_k(x)$ are grouped, \n",
    "            <li> $E_D(\\tilde{w}) = \\frac{1}{2} \\mathrm{Tr}\\{(\\tilde{W}^T\\tilde{X} - T)^T(\\tilde{W}^T\\tilde{X} - T)\\}$.\n",
    "            <li> $\\tilde{W} = (X^TX)^{-1}X^TT = X^{+}T$, where $T$ is $N x K$ matrix where $n^{th}$ row is $t_n^T$\n",
    "            <li> Too sensitive to outliers.\n",
    "        </ul><br>\n",
    "        <li><b>2) Fisher's Linear Discriminant:</b>\n",
    "            <ul><li> A way to view a linear classification model is in terms of dimensionality reduction.\n",
    "                <li> $ Y = W^TX$ is a projection from $X$-space to $Y$-space through the $W$\n",
    "                <li> We need to find the projection that maximizes the between-classes variance and minimizes the within-class variance\n",
    "            </ul><br>\n",
    "        <li><b>3) The perceptron:</b>\n",
    "            <ul><li> Only for two-class classification.\n",
    "                <li> Use the target coding scheme $\\{-1, +1\\}$.\n",
    "                <li> $ Y = f(W^T\\phi(X)) \\begin{equation} = \\begin{cases} +1 & \\text{,if } y(x) \\geq 0\\\\\n",
    "                                                                            -1 & \\text{,if } y(x) < 0\n",
    "                                                                            \\end{cases}\n",
    "                                                                            \\end{equation}$.\n",
    "                <br><br><li> $E_p(w) = -\\sum_{n\\in M} W^T\\;\\phi(x_n)\\;t_n$ ,where $M$ is a set of all misclassified patterns.\n",
    "                <br><li> $W^{(\\tau+1)} = W^{\\tau} - \\eta\\;\\nabla\\;E_p(w) = W^{\\tau} + \\eta\\;\\phi(x_n)\\;t_n.$\n",
    "                <br><li> So we cycle through the training dataset, and evaluate the $y(x)$ for each $x_n$, if:\n",
    "                    <ul><li> The pattern $x_n$ is correctly classified, w is unchanged.\n",
    "                        <li> The pattern $x_n$ is incorrectly classified: $\\begin{equation}\\begin{cases} Add\\;\\;\\;\\;\\;\\;\\;\\; \\phi(x_n) \\;\\;if\\;\\;class\\;\\;1\\\\ Subtract\\;\\;\\phi(x_n)\\;\\;if\\;\\;class\\;\\;0 \\end{cases} \\end{equation}$\n",
    "         -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb3ad8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\"></hr>\n",
    "\n",
    "### 1) Discriminant Function<br>\n",
    "\n",
    "Assign $x => C_K$\n",
    "All decision boundaries are linear, singly connected, and convex.\n",
    "#### For two-class problem:\n",
    "- $y = W^TX + W_0$ $\\begin{equation} C_K= \\begin{cases} 1 & \\text{,if } y(x) \\geq 0\\\\ 0 & \\text{,if } y(x) < 0 \\end{cases} \\end{equation}$\n",
    "- $W$ determine the orientation of the D.B.\n",
    "- $W_0$ determines the location of the D.B.\n",
    "- $y(x)$ gives a signed measure of the perpendicular distance of x from the D.B.\n",
    "\n",
    "#### For multi-class problem:\n",
    "- Can be done by two main approaches:\n",
    "    - 1) Using 2-class classifiers:\n",
    "        - Leads to ambiguous.\n",
    "        - ex. One-Versus-The-Rest or One-Versus-One.\n",
    "        \n",
    "    - 2) Using a single k-class discriminant comprising K linear functions\"\n",
    "        - $y_k(x) = W^T_kX + W_{k0}$, $y(x) = (y_1(x),...,y_K(x))$.\n",
    "        - Assign $X$ to $C_K \\text{ where } y_k(x) > y_j(x) \\ \\ \\forall_{j\\neq k}$\n",
    "        - The D.B between $C_k$ and $C_j$ is $y_k(x) = y_j(x)$</li>\n",
    "          \n",
    "\n",
    "#### To find the parameters $W$ in discriminant function, we have three methods:\n",
    "- 1) Least-Squares:\n",
    "    - $y(x) = \\tilde{W}^T\\tilde{X}$, where $\\tilde{W} = (W_{k0}, W_{k})$ and $\\tilde{X} = (1, X)$ and all classes models $y_k(x)$ are grouped, \n",
    "    - $E_D(\\tilde{w}) = \\frac{1}{2} \\mathrm{Tr}\\{(\\tilde{W}^T\\tilde{X} - T)^T(\\tilde{W}^T\\tilde{X} - T)\\}$.\n",
    "    - $\\tilde{W} = (X^TX)^{-1}X^TT = X^{+}T$, where $T$ is $N x K$ matrix where $n^{th}$ row is $t_n^T$\n",
    "    - Too sensitive to outliers.\n",
    "    \n",
    "    \n",
    "- 2) Fisher's Linear Discriminant:\n",
    "    - A way to view a linear classification model is in terms of dimensionality reduction.\n",
    "    - $ Y = W^TX$ is a projection from $X$-space to $Y$-space through the $W$\n",
    "    - We need to find the projection that maximizes the between-classes variance and minimizes the within-class variance.\n",
    "    \n",
    "    \n",
    "- 3) The perceptron:</b>\n",
    "    - Only for two-class classification.\n",
    "    - Use the target coding scheme $\\{-1, +1\\}$.\n",
    "    - $ Y = f(W^T\\phi(X)) \\begin{equation} = \\begin{cases} +1 & \\text{,if } y(x) \\geq 0\\\\ -1 & \\text{,if } y(x) < 0 \\end{cases} \\end{equation}$.\n",
    "    - $E_p(w) = -\\sum_{n\\in M} W^T\\;\\phi(x_n)\\;t_n$ ,where $M$ is a set of all misclassified patterns.\n",
    "    - $W^{(\\tau+1)} = W^{\\tau} - \\eta\\;\\nabla\\;E_p(w) = W^{\\tau} + \\eta\\;\\phi(x_n)\\;t_n.$\n",
    "    - So we cycle through the training dataset, and evaluate the $y(x)$ for each $x_n$, if:\n",
    "        - The pattern $x_n$ is correctly classified, w is unchanged.\n",
    "        - The pattern $x_n$ is incorrectly classified: $\\begin{equation}\\begin{cases} Add\\;\\;\\;\\;\\;\\;\\;\\; \\phi(x_n)\\;\\;if\\;\\;class\\;\\;1\\\\ Subtract\\;\\;\\phi(x_n)\\;\\;if\\;\\;class\\;\\;0 \\end{cases} \\end{equation}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
